import requests
from config import supabase
from datetime import datetime

# Helper to fetch the integration record
def get_integration(user_id: str):
    response = supabase.table("integrations").select("id, access_token").eq("user_id", user_id).eq("provider", "google_drive").single().execute()
    return response.data

def index_google_drive(user_id: str):
    print(f"--- Starting Indexing for {user_id} ---")
    
    try:
        # 1. Update status to syncing
        supabase.table("integrations").update({"sync_status": "syncing"}).eq("user_id", user_id).eq("provider", "google_drive").execute()

        # 2. Get Credentials & Integration ID
        integration = get_integration(user_id)
        access_token = integration['access_token']
        integration_id = integration['id']

        headers = {"Authorization": f"Bearer {access_token}"}
        
        # 3. List Files (Google Docs only for MVP)
        q = "mimeType='application/vnd.google-apps.document' and trashed=false"
        res = requests.get(f"https://www.googleapis.com/drive/v3/files?q={q}&fields=files(id, name, mimeType, webViewLink, createdTime, modifiedTime)", headers=headers)
        files = res.json().get('files', [])

        for file in files:
            print(f"Processing: {file['name']}")

            # 4. Fetch Content (Export as Plain Text)
            # In the future, use 'markitdown' here for better formatting
            content_url = f"https://www.googleapis.com/drive/v3/files/{file['id']}/export?mimeType=text/plain"
            content_res = requests.get(content_url, headers=headers)
            text_content = content_res.text

            # 5. Insert into 'documents' (The Master Record)
            # We use upsert to avoid duplicates if we run this twice
            doc_data = {
                "user_id": user_id,
                "integration_id": integration_id,
                "external_id": file['id'],
                "name": file['name'],
                "mime_type": file['mimeType'],
                "url": file['webViewLink'],
                "created_at_external": file['createdTime'],
                "modified_at_external": file['modifiedTime'],
                "last_synced_at": datetime.utcnow().isoformat()
            }
            
            # Upsert checks for conflict on ID, but we want to check on (integration_id, external_id)
            # Since you don't have a unique constraint there yet, we'll do a check or just Insert.
            # Let's assuming we insert for now. 
            
            doc_res = supabase.table("documents").insert(doc_data).execute()
            doc_id = doc_res.data[0]['id']

            # 6. Insert into 'document_chunks' (The Searchable Content)
            # For MVP, we dump the whole text into one chunk. 
            # Later, you can split this text if it's > 4000 chars.
            chunk_data = {
                "document_id": doc_id,
                "content": text_content,
                "chunk_index": 0
                # 'fts' is auto-generated by your SQL
                # 'embedding' is null for now (we will add AI later)
            }
            supabase.table("document_chunks").insert(chunk_data).execute()

        # 7. Success
        supabase.table("integrations").update({"sync_status": "success"}).eq("id", integration_id).execute()
        print("--- Indexing Complete ---")

    except Exception as e:
        print(f"Indexing Failed: {e}")
        supabase.table("integrations").update({"sync_status": "error"}).eq("user_id", user_id).execute()